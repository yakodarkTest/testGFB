{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "988252ba",
   "metadata": {},
   "source": [
    "# Ejercicio 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d75bd3a",
   "metadata": {},
   "source": [
    "### Consideraciones  Previa\n",
    "-El Ejercicio se trabajo de manera local para su posterior ejecucion en en entorno de AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec3081a",
   "metadata": {},
   "source": [
    "### Creacion de Bucket en S3  \n",
    "#### Para este Punto ya se creo la cuenta en AWS para el usuario user-1\n",
    "#### Librerias Necesarias Requisitios : \n",
    "-**boto3  :** makes it easy to integrate you Python application, library or script with AWS services. It allows Python developers to write softare that makes use of services like Amazon S3 and Amazon EC2.\n",
    "\\\n",
    "-**awswrangler :** An open-source Python package that extends the power of Pandas library to AWS connecting DataFrames and AWS data related services.An open-source Python package that extends the power of Pandas library to AWS connecting DataFrames and AWS data related services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2645e9ae",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "842fdb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Package(s) not found: awswrangler\u001b[0m\n",
      "Name: boto3\n",
      "Version: 1.20.25\n",
      "Summary: The AWS SDK for Python\n",
      "Home-page: https://github.com/boto/boto3\n",
      "Author: Amazon Web Services\n",
      "Author-email: \n",
      "License: Apache License 2.0\n",
      "Location: /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages\n",
      "Requires: botocore, jmespath, s3transfer\n",
      "Required-by: sagemaker, smclarify, smdebug\n"
     ]
    }
   ],
   "source": [
    "\"\"\"PASO 0\n",
    "Validamos que existan las libreria necesarias\"\"\"\n",
    "!pip show boto3 awswrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4411321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytest-shutil\n",
      "  Downloading pytest_shutil-1.7.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting path.py\n",
      "  Downloading path.py-12.5.0-py3-none-any.whl (2.3 kB)\n",
      "Requirement already satisfied: contextlib2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pytest-shutil) (0.6.0.post1)\n",
      "Collecting execnet\n",
      "  Downloading execnet-1.9.0-py2.py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: mock in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pytest-shutil) (4.0.3)\n",
      "Requirement already satisfied: pytest in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pytest-shutil) (6.2.2)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pytest-shutil) (1.15.0)\n",
      "Requirement already satisfied: path in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from path.py->pytest-shutil) (15.1.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pytest->pytest-shutil) (20.3.0)\n",
      "Requirement already satisfied: iniconfig in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pytest->pytest-shutil) (1.1.1)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pytest->pytest-shutil) (21.3)\n",
      "Requirement already satisfied: pluggy<1.0.0a1,>=0.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pytest->pytest-shutil) (0.13.1)\n",
      "Requirement already satisfied: py>=1.8.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pytest->pytest-shutil) (1.10.0)\n",
      "Requirement already satisfied: toml in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pytest->pytest-shutil) (0.10.2)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pytest->pytest-shutil) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=0.12->pytest->pytest-shutil) (4.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=0.12->pytest->pytest-shutil) (3.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging->pytest->pytest-shutil) (2.4.7)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=a9582c38b2050280087199ef6c0c9f80b77b0880516322f057561879c8c50321\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "Successfully built termcolor\n",
      "Installing collected packages: termcolor, path.py, execnet, pytest-shutil\n",
      "Successfully installed execnet-1.9.0 path.py-12.5.0 pytest-shutil-1.7.0 termcolor-1.1.0\n",
      "Name: boto3\n",
      "Version: 1.20.25\n",
      "Summary: The AWS SDK for Python\n",
      "Home-page: https://github.com/boto/boto3\n",
      "Author: Amazon Web Services\n",
      "Author-email: \n",
      "License: Apache License 2.0\n",
      "Location: /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages\n",
      "Requires: botocore, jmespath, s3transfer\n",
      "Required-by: awswrangler, redshift-connector, sagemaker, smclarify, smdebug\n",
      "---\n",
      "Name: awswrangler\n",
      "Version: 2.13.0\n",
      "Summary: Pandas on AWS.\n",
      "Home-page: https://aws-data-wrangler.readthedocs.io/\n",
      "Author: Igor Tavares\n",
      "Author-email: \n",
      "License: Apache-2.0\n",
      "Location: /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages\n",
      "Requires: boto3, botocore, jsonpath-ng, numpy, openpyxl, opensearch-py, pandas, pg8000, progressbar2, pyarrow, pymysql, redshift-connector, requests-aws4auth, xlrd, xlwt\n",
      "Required-by: \n",
      "---\n",
      "Name: pytest-shutil\n",
      "Version: 1.7.0\n",
      "Summary: A goodie-bag of unix shell and environment tools for py.test\n",
      "Home-page: https://github.com/manahl/pytest-plugins\n",
      "Author: Edward Easton\n",
      "Author-email: eeaston@gmail.com\n",
      "License: MIT license\n",
      "Location: /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages\n",
      "Requires: contextlib2, execnet, mock, path.py, pytest, six, termcolor\n",
      "Required-by: \n",
      "Fin del Proceso\n"
     ]
    }
   ],
   "source": [
    "\"\"\"PASO 0.1 Si nos existe algun paquete aplicar el sigueinte procedimiento \"\"\"\n",
    "## Ojo --> Descomentar paso por paso para su Ejecucion\n",
    "#__1 Actualizar la libreria pip\n",
    "#!pip install --upgrade pip\n",
    "#__2 Instalar la libreria faltante\n",
    "#!pip install boto3\n",
    "#!pip install awswrangler\n",
    "#!pip install pytest-shutil\n",
    "#!conda install -c auto logging\n",
    "#__3 Validar que se encuentra instalo\n",
    "!pip show boto3 awswrangler pytest-shutil\n",
    "print(\"Fin del Proceso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde44669",
   "metadata": {},
   "source": [
    "### Funciones para los procesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "042d77c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funciones Compiladas\n"
     ]
    }
   ],
   "source": [
    "\"\"\"PASO 0.3 >>>>> COMPILAR Funciones y Class necesarias para los siguientes pasos\"\"\"\n",
    "#Funcion para Crear Bucket\n",
    "#Referencia : https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-example-creating-buckets.html \n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "def S3_create_bucket(bucket_name, region=None):\n",
    "    # Create bucket\n",
    "    try:\n",
    "        if region is None:\n",
    "            # Credenciaales creadas para el usuario user-test \n",
    "            s3_client = boto3.client('s3')\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            s3_client = boto3.client('s3', region_name=region)\n",
    "            location = {'LocationConstraint': region}\n",
    "            s3_client.create_bucket(Bucket=bucket_name,\n",
    "                                    CreateBucketConfiguration=location)\n",
    "    except ClientError as e:\n",
    "        e. __str__ \n",
    "        #logging.error(e)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "#Classe Crear Carpetas en Bucket\n",
    "#Referencia : https://stackoverflow.com/questions/1939743/amazon-s3-boto-how-to-create-a-folder\n",
    "from boto3 import client, resource\n",
    "class S3Helper:\n",
    "    def __init__(self):\n",
    "        self.client = client(\"s3\")\n",
    "        self.s3 = resource('s3')\n",
    "\n",
    "    def create_folder(self, path):\n",
    "        path_arr = path.rstrip(\"/\").split(\"/\")\n",
    "        if len(path_arr) == 1:\n",
    "            return self.client.create_bucket(Bucket=path_arr[0])\n",
    "        parent = path_arr[0]\n",
    "        bucket = self.s3.Bucket(parent)\n",
    "        status = bucket.put_object(Key=\"/\".join(path_arr[1:]) + \"/\")\n",
    "        return status\n",
    "#Funcion Cargar Archivo a S3 \n",
    "#Referencia : https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-uploading-files.html\n",
    "def S3_upload_file(file_name, bucket, object_name=None):\n",
    "    \"\"\"Upload a file to an S3 bucket\n",
    "\n",
    "    :param file_name: File to upload\n",
    "    :param bucket: Bucket to upload to\n",
    "    :param object_name: S3 object name. If not specified then file_name is used\n",
    "    :return: True if file was uploaded, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # If S3 object_name was not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = os.path.basename(file_name)\n",
    "\n",
    "    # Upload the file\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        response = s3_client.upload_file(file_name, bucket, object_name)\n",
    "    except ClientError as e:\n",
    "        e. __str__ \n",
    "        #logging.error(e)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Funciones Compiladas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b461d7c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' PASO II Creacion de Bucket\\nSe configuro en AWS Cloudtrail , las comfiguracion solicitadas\\n '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" PASO II Creacion de Bucket\n",
    "Se configuro en AWS Cloudtrail , las comfiguracion solicitadas\n",
    " \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a444227d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se creo el Bucket :anoc001-test-data-mauriciobarojas\n"
     ]
    }
   ],
   "source": [
    "\"\"\"III Creacion de Bucket \n",
    "    Rerencia : https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-example-creating-buckets.html\n",
    "    >>>>>>>>>>>>>>>>>>   ESTE PASO NO SE PUDO EJECUTAR EN LA NOTEBOOK SAGEMAKER , SE CREO DE FORMA MANUAL el Bucket   <<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\"\"\"\n",
    "#Nombre de los Bucket a crear    \n",
    "import boto3\n",
    "#Se declaran las credenciales para acceder al S3\n",
    "\"\"\"Se eliminaron las Credeniasles utilizadas para el ejercio    si se requiere replicar  ,por favor ingresar credenciales \"\"\"\n",
    "#boto3.Session(aws_access_key_id='aaaaaaaaaaaaaaaaaaaa',aws_secret_access_key='bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb')\n",
    "\n",
    "#Nombre de los Bucket a crear\n",
    "bucket1= 'anoc001-test-data-mauriciobarojas'\n",
    "\n",
    "#Crea los Buckets en el S3 llamando a la fucion create_bucket\n",
    "try:\n",
    "    S3_create_bucket(bucket1,'us-east-1')\n",
    "    print(\"Se creo el Bucket :\" + bucket1)\n",
    "except Exception as inst:\n",
    "    #print(type(inst))    # the exception instance \n",
    "    #print(inst.args)     # arguments stored in .args\n",
    "    print(inst)          # __str__ allows args to be printed directly,      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c02df451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buckets Existentes:\n",
      "  anoc001-test-data-mauriciobarojas\n",
      "  anoc001-test-mauriciobarojas\n",
      "  previo211230\n"
     ]
    }
   ],
   "source": [
    "\"\"\"VALIDACION PASO II Y III Lista los Bucket Existentes\"\"\"\n",
    "s3 = boto3.client('s3')\n",
    "response = s3.list_buckets()\n",
    "print('Buckets Existentes:')\n",
    "for bucket in response['Buckets']:\n",
    "    print(f'  {bucket[\"Name\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fdab802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se creo la carpeta : anoc001-test-data-mauriciobarojas/raw\n",
      "Se creo la carpeta : anoc001-test-data-mauriciobarojas/processed\n"
     ]
    }
   ],
   "source": [
    "\"\"\"PASO III Creacion de Carpeta en Bucket \n",
    "    Rerencia : https://stackoverflow.com/questions/1939743/amazon-s3-boto-how-to-create-a-folder\n",
    "\"\"\"\n",
    "bucket_name= 'anoc001-test-data-mauriciobarojas' # nombre del bucket\n",
    "try:\n",
    "    s3 = S3Helper()\n",
    "    s3.create_folder(bucket_name+'/'+'raw')\n",
    "    print(\"Se creo la carpeta : \" + bucket_name+'/'+'raw')\n",
    "    s3.create_folder(bucket_name+'/'+'processed')\n",
    "    print(\"Se creo la carpeta : \" + bucket_name+'/'+'processed')\n",
    "except Exception as inst:\n",
    "    print(inst)          # __str__ allows args to be printed directly,   \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5a8e780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se cargo el Archivo al Bucket :anoc001-test-data-mauriciobarojas/raw/data_prueba_tecnica.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\" PASO IV Carga Archivo data_prueba_tecnica.csv a raw \n",
    "    Referencia : https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-uploading-files.html\n",
    "\"\"\"\n",
    "import os\n",
    "bucket_name= 'anoc001-test-data-mauriciobarojas' # nombre del bucket\n",
    "try:\n",
    "    #Es necesario tener el archivo en la raiz del proyecto de Jupyter Lapzxz \n",
    "    salida = S3_upload_file(os.path.join('./','data_prueba_tecnica.csv'), bucket_name,'raw/data_prueba_tecnica.csv')\n",
    "    if salida == True:        print(\"Se cargo el Archivo al Bucket :\" + bucket_name + '/raw/data_prueba_tecnica.csv')       \n",
    "except Exception as inst:\n",
    "    print(inst)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e54ee7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PASO V . B : Seleccion de Ambiente : Notebook de Sagemaker \n",
    "-**1er**  motivo es que esta una forma sencilla de tener el entorno ya preconfigurado de Jupyter Lap\n",
    "\\\n",
    "-**2do** motivo el costo de de utilizar Notebook Sagemaker VS una intancia linux o windows y configurar Jupyter Lap\n",
    "\\\n",
    "-**3er** motivo se evita la configuracion del entorno en la Intancia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61904991",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>company_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>status</th>\n",
       "      <th>created_at</th>\n",
       "      <th>paid_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48ba4bdbfb56ceebb32f2bd0263e759be942af3d</td>\n",
       "      <td>MiPasajefy</td>\n",
       "      <td>cbf1c8b09cd5b549416d49d220a40cbd317f952e</td>\n",
       "      <td>3.0</td>\n",
       "      <td>voided</td>\n",
       "      <td>2019-03-19</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>05fc6f5ac66b6ee7e4253aa5d0c2299eb47aaaf4</td>\n",
       "      <td>MiPasajefy</td>\n",
       "      <td>cbf1c8b09cd5b549416d49d220a40cbd317f952e</td>\n",
       "      <td>3.0</td>\n",
       "      <td>pending_payment</td>\n",
       "      <td>2019-05-06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2cdce231c1fc6a2061bfa2f1d978351fe217245d</td>\n",
       "      <td>MiPasajefy</td>\n",
       "      <td>cbf1c8b09cd5b549416d49d220a40cbd317f952e</td>\n",
       "      <td>3.0</td>\n",
       "      <td>voided</td>\n",
       "      <td>2019-02-22</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>81633ba310a50b673efd469c37139576982901aa</td>\n",
       "      <td>MiPasajefy</td>\n",
       "      <td>cbf1c8b09cd5b549416d49d220a40cbd317f952e</td>\n",
       "      <td>102.61</td>\n",
       "      <td>paid</td>\n",
       "      <td>2019-02-27</td>\n",
       "      <td>2019-02-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ccfc4c24e788e4bca448df343698782db6b0c0b</td>\n",
       "      <td>MiPasajefy</td>\n",
       "      <td>cbf1c8b09cd5b549416d49d220a40cbd317f952e</td>\n",
       "      <td>184.49</td>\n",
       "      <td>paid</td>\n",
       "      <td>2019-02-05</td>\n",
       "      <td>2019-02-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>4907c36c037bfa4874047e14f722329016dc3908</td>\n",
       "      <td>MiPasajefy</td>\n",
       "      <td>cbf1c8b09cd5b549416d49d220a40cbd317f952e</td>\n",
       "      <td>59.88</td>\n",
       "      <td>paid</td>\n",
       "      <td>2019-03-14</td>\n",
       "      <td>2019-03-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>ef9ec85887418d2ccf220686312cfc4c615665d5</td>\n",
       "      <td>MiPasajefy</td>\n",
       "      <td>cbf1c8b09cd5b549416d49d220a40cbd317f952e</td>\n",
       "      <td>23.05</td>\n",
       "      <td>paid</td>\n",
       "      <td>2019-02-20</td>\n",
       "      <td>2019-02-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>50bb3950ecde94ca3e07853003a11a413508da36</td>\n",
       "      <td>MiPasajefy</td>\n",
       "      <td>cbf1c8b09cd5b549416d49d220a40cbd317f952e</td>\n",
       "      <td>20.5</td>\n",
       "      <td>paid</td>\n",
       "      <td>2019-03-19</td>\n",
       "      <td>2019-03-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>1b2c4e503a4a952064b70369cf8e72937c720474</td>\n",
       "      <td>MiPasajefy</td>\n",
       "      <td>cbf1c8b09cd5b549416d49d220a40cbd317f952e</td>\n",
       "      <td>81.42</td>\n",
       "      <td>paid</td>\n",
       "      <td>2019-03-10</td>\n",
       "      <td>2019-03-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>4e85c4eac968c9465fc8d34bcd4968ac019fa850</td>\n",
       "      <td>MiPasajefy</td>\n",
       "      <td>cbf1c8b09cd5b549416d49d220a40cbd317f952e</td>\n",
       "      <td>33.45</td>\n",
       "      <td>paid</td>\n",
       "      <td>2019-03-16</td>\n",
       "      <td>2019-03-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            id        name  \\\n",
       "0     48ba4bdbfb56ceebb32f2bd0263e759be942af3d  MiPasajefy   \n",
       "1     05fc6f5ac66b6ee7e4253aa5d0c2299eb47aaaf4  MiPasajefy   \n",
       "2     2cdce231c1fc6a2061bfa2f1d978351fe217245d  MiPasajefy   \n",
       "3     81633ba310a50b673efd469c37139576982901aa  MiPasajefy   \n",
       "4     6ccfc4c24e788e4bca448df343698782db6b0c0b  MiPasajefy   \n",
       "...                                        ...         ...   \n",
       "9995  4907c36c037bfa4874047e14f722329016dc3908  MiPasajefy   \n",
       "9996  ef9ec85887418d2ccf220686312cfc4c615665d5  MiPasajefy   \n",
       "9997  50bb3950ecde94ca3e07853003a11a413508da36  MiPasajefy   \n",
       "9998  1b2c4e503a4a952064b70369cf8e72937c720474  MiPasajefy   \n",
       "9999  4e85c4eac968c9465fc8d34bcd4968ac019fa850  MiPasajefy   \n",
       "\n",
       "                                    company_id  amount           status  \\\n",
       "0     cbf1c8b09cd5b549416d49d220a40cbd317f952e     3.0           voided   \n",
       "1     cbf1c8b09cd5b549416d49d220a40cbd317f952e     3.0  pending_payment   \n",
       "2     cbf1c8b09cd5b549416d49d220a40cbd317f952e     3.0           voided   \n",
       "3     cbf1c8b09cd5b549416d49d220a40cbd317f952e  102.61             paid   \n",
       "4     cbf1c8b09cd5b549416d49d220a40cbd317f952e  184.49             paid   \n",
       "...                                        ...     ...              ...   \n",
       "9995  cbf1c8b09cd5b549416d49d220a40cbd317f952e   59.88             paid   \n",
       "9996  cbf1c8b09cd5b549416d49d220a40cbd317f952e   23.05             paid   \n",
       "9997  cbf1c8b09cd5b549416d49d220a40cbd317f952e    20.5             paid   \n",
       "9998  cbf1c8b09cd5b549416d49d220a40cbd317f952e   81.42             paid   \n",
       "9999  cbf1c8b09cd5b549416d49d220a40cbd317f952e   33.45             paid   \n",
       "\n",
       "      created_at     paid_at  \n",
       "0     2019-03-19         NaN  \n",
       "1     2019-05-06         NaN  \n",
       "2     2019-02-22         NaN  \n",
       "3     2019-02-27  2019-02-27  \n",
       "4     2019-02-05  2019-02-05  \n",
       "...          ...         ...  \n",
       "9995  2019-03-14  2019-03-14  \n",
       "9996  2019-02-20  2019-02-20  \n",
       "9997  2019-03-19  2019-03-19  \n",
       "9998  2019-03-10  2019-03-10  \n",
       "9999  2019-03-16  2019-03-17  \n",
       "\n",
       "[10000 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" PASO IV.1 Se lee el archivo cargado directamente del Bucket \"\"\"\n",
    "#leer el archivo cargado al bucket \n",
    "#Se emplea la libreria awswrangler para poder leer el archivo cargado al Bucket\n",
    "import awswrangler  as wr\n",
    "\n",
    "bucket_name= 'anoc001-test-data-mauriciobarojas' # nombre del bucket\n",
    "\n",
    "s3=boto3.client('s3')\n",
    "wr.s3.list_directories('s3://'+ bucket_name)\n",
    "data = wr.s3.read_csv(path='s3://'+ bucket_name +'/raw/'+ 'data_prueba_tecnica.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b82e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PASO VI Resultado del Analisis de : **data_prueba_tecnica**\n",
    "### Se  analizan las columna id :\n",
    "-Se detecta que se tienen 3 registros con id nulo    \n",
    "### Se  analizan las columnas name y company_id :\n",
    "-**Name :** Se detecta que tiene registros con caracteres hexadecimales : , **MiPas0xFFFF, MiP0xFFFF**\n",
    "\\\n",
    "-**Company_id :** Se detencta un registro con  asteriscos : **'*******'**\n",
    "### Se analiza el resto de las columnas  y solo se  identifica que la columna Status: \n",
    "-Contiene 2 registros con las siguientes descripciones  :  **0xFFFF , p&0x3fid**\n",
    "### Acciones para los campos  id, name,compay_id y status\n",
    "-Se ailaran en un dataset para su posterior analisis y no se cargue data imcompleta\n",
    "\\\n",
    "### Respecto al campo paid_at\n",
    "-Aparenteme contiene 39991 registros Nulos , pero este este valor es correcto por que se encuentra agrupado en los status  : **expired, pending_payment, pre_authorized y voided**\n",
    "\n",
    "El resto de los campos ,no presenta ningún valor extraño\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb6b1efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               3\n",
       "name             3\n",
       "company_id       4\n",
       "amount           0\n",
       "status           0\n",
       "created_at       0\n",
       "paid_at       3991\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "PASO VI.A.1\n",
    "\"\"\"\n",
    "#Con la primesa de que existen Nulos en id se realiza una busqueda general de NAN o Nulos\n",
    "data.isna().sum()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04216e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se archiv data_a_validar.csv con exito\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PASO VI.A.2\n",
    "PASO VI.B\n",
    "PASO VI.C\n",
    "\"\"\"\n",
    "#Despues de Realizar el Analisis se aislan estos registros para validar el origen de los mismos\n",
    "try:\n",
    "    filtro1 = ['0xFFFF','p&0x3fid']\n",
    "    filtro2 = ['MiPas0xFFFF','MiP0xFFFF']\n",
    "    dataValidar = data[(data['id'].isnull()) | (data['company_id'] == '*******')| data.status.isin(filtro1)| data.name.isin(filtro2) ]\n",
    "    #Se exporta aun archivo CSV \n",
    "    dataValidar.to_csv( 'data_a_validar.csv')\n",
    "    print('Se archiv data_a_validar.csv con exito')    \n",
    "except Exception as inst:\n",
    "    print(inst)          # __str__ allows args to be printed directly,      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "788682c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se exporto data_limpia.csv con exito\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PASO VI : Data Set Depurado\n",
    "\"\"\"\n",
    "#Se borran estos registros del generar el DataSet que se cargara al Amazon QuickSight\n",
    "#Se eliminan del dataframe inicial los registros de ID que tienen NAN , las otras datos a tipicos\n",
    "try:\n",
    "    #filtro = ['0xFFFF','p&0x3fid']\n",
    "    #filtro2 = ['MiPas0xFFFF','MiP0xFFFF']\n",
    "    #Se eliminan los nulos de id\n",
    "    data= data.dropna(subset=['id'])  \n",
    "    #Se eliminan el resto de registros atipicos\n",
    "    data= data[(data['company_id'] != '*******') &(~data.status.isin(filtro1)) &(~data.name.isin(filtro2))] \n",
    "    data.to_csv('data_limpia.csv')\n",
    "    print('Se exporto data_limpia.csv con exito')  \n",
    "except Exception as inst:\n",
    "    print(inst)          # __str__ allows args to be printed directly,      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b7159ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se cargo el Archivo al Bucket :anoc001-test-data-mauriciobarojas/processed/data_limpia.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\" PASO VI.Carga de data_limpia.csv en el Bucket\n",
    "     Carga Archivo data_prueba_tecnica.csv a data \n",
    "    Referencia : https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-uploading-files.html\n",
    "\"\"\"\n",
    "import os\n",
    "bucket_name= 'anoc001-test-data-mauriciobarojas' # nombre del bucket\n",
    "\n",
    "try:\n",
    "    #Es necesario tener el archivo en la raiz del proyecto de Jupyter Lapzxz \n",
    "    salida = S3_upload_file(os.path.join('./','data_limpia.csv'), bucket_name,'processed/data_limpia.csv')\n",
    "    if salida == True:        print(\"Se cargo el Archivo al Bucket :\" + bucket_name + '/processed/data_limpia.csv')       \n",
    "except Exception as inst:\n",
    "    print(inst)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5a5045a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se cargo el Archivo al Bucket :anoc001-test-data-mauriciobarojas/processed/data_a_validar.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\" PASO VI.Carga de data_a_validar.csv en el Bucket\n",
    "    Carga Archivo data_a_validar.csv a data \n",
    "    Referencia : https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-uploading-files.html\n",
    "\"\"\"\n",
    "import os\n",
    "#Se declaran las credenciales para acceder al S3\n",
    "bucket_name= 'anoc001-test-data-mauriciobarojas' # nombre del bucket\n",
    "\n",
    "try:\n",
    "    #Es necesario tener el archivo en la raiz del proyecto de Jupyter Lapzxz \n",
    "    salida = S3_upload_file(os.path.join('./','data_a_validar.csv'), bucket_name,'processed/data_a_validar.csv')\n",
    "    if salida == True:        print(\"Se cargo el Archivo al Bucket :\" + bucket_name + '/processed/data_a_validar.csv')       \n",
    "except Exception as inst:\n",
    "    print(inst)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72090163",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalizo la descarga del Bucket : anoc001-test-data-mauriciobarojas\n",
      "Se creo zip del Bucket : anoc001-test-data-mauriciobarojas\n",
      "Se limpio la carpeta : anoc001-test-data-mauriciobarojas\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail-Digest/af-south-1/2021/12/30/859569241075_CloudTrail-Digest_af-south-1_Trial-managementEVent_us-east-2_20211230T150545Z.json.gz.a9dfB0AE'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail-Digest/ap-east-1/2021/12/30/859569241075_CloudTrail-Digest_ap-east-1_Trial-managementEVent_us-east-2_20211230T150545Z.json.gz.bb38F39A'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail-Digest/ap-northeast-1/2021/12/30/859569241075_CloudTrail-Digest_ap-northeast-1_Trial-managementEVent_us-east-2_20211230T150545Z.json.gz.fd26cab0'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail-Digest/ap-northeast-2/2021/12/30/859569241075_CloudTrail-Digest_ap-northeast-2_Trial-managementEVent_us-east-2_20211230T150545Z.json.gz.A1E24f89'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail-Digest/ap-northeast-3/2021/12/30/859569241075_CloudTrail-Digest_ap-northeast-3_Trial-managementEVent_us-east-2_20211230T150545Z.json.gz.d7ccfBDE'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail-Digest/ap-south-1/2021/12/30/859569241075_CloudTrail-Digest_ap-south-1_Trial-managementEVent_us-east-2_20211230T150545Z.json.gz.b8cbcbEf'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail-Digest/ap-southeast-1/2021/12/30/859569241075_CloudTrail-Digest_ap-southeast-1_Trial-managementEVent_us-east-2_20211230T150123Z.json.gz.9f9B6bc1'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail-Digest/ap-southeast-2/2021/12/30/859569241075_CloudTrail-Digest_ap-southeast-2_Trial-managementEVent_us-east-2_20211230T150545Z.json.gz.8C2De5bE'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail-Digest/ap-southeast-3/2021/12/30/859569241075_CloudTrail-Digest_ap-southeast-3_Trial-managementEVent_us-east-2_20211230T150545Z.json.gz.Bac63BaB'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail-Digest/ca-central-1/2021/12/30/859569241075_CloudTrail-Digest_ca-central-1_Trial-managementEVent_us-east-2_20211230T150545Z.json.gz.cCD22728'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail-Digest/eu-central-1/2021/12/30/859569241075_CloudTrail-Digest_eu-central-1_Trial-managementEVent_us-east-2_20211230T150545Z.json.gz.cAec41BD'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail-Digest/eu-north-1/2021/12/30/859569241075_CloudTrail-Digest_eu-north-1_Trial-managementEVent_us-east-2_20211230T150545Z.json.gz.3c6Cd4BF'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail-Digest/eu-south-1/2021/12/30/859569241075_CloudTrail-Digest_eu-south-1_Trial-managementEVent_us-east-2_20211230T150545Z.json.gz.bDd1a2F0'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail-Digest/eu-west-1/2021/12/30/859569241075_CloudTrail-Digest_eu-west-1_Trial-managementEVent_us-east-2_20211230T150545Z.json.gz.5BE9DDED'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail-Digest/eu-west-2/2021/12/30/859569241075_CloudTrail-Digest_eu-west-2_Trial-managementEVent_us-east-2_20211230T150545Z.json.gz.6F49dfdA'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail-Digest/eu-west-3/2021/12/30/859569241075_CloudTrail-Digest_eu-west-3_Trial-managementEVent_us-east-2_20211230T150545Z.json.gz.8A2a9B6c'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail-Digest/me-south-1/2021/12/30/859569241075_CloudTrail-Digest_me-south-1_Trial-managementEVent_us-east-2_20211230T150123Z.json.gz.EdCBCeef'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail-Digest/sa-east-1/2021/12/30/859569241075_CloudTrail-Digest_sa-east-1_Trial-managementEVent_us-east-2_20211230T150545Z.json.gz.E0b2D5CE'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail-Digest/us-east-1/2021/12/30/859569241075_CloudTrail-Digest_us-east-1_Trial-managementEVent_us-east-2_20211230T150545Z.json.gz.e09B98AA'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail-Digest/us-east-2/2021/12/30/859569241075_CloudTrail-Digest_us-east-2_Trial-managementEVent_us-east-2_20211230T150545Z.json.gz.cC1ff5Ad'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail-Digest/us-west-1/2021/12/30/859569241075_CloudTrail-Digest_us-west-1_Trial-managementEVent_us-east-2_20211230T150545Z.json.gz.ebF11Af2'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail-Digest/us-west-2/2021/12/30/859569241075_CloudTrail-Digest_us-west-2_Trial-managementEVent_us-east-2_20211230T150545Z.json.gz.C9ACd416'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail/us-east-1/2021/12/30/859569241075_CloudTrail_us-east-1_20211230T1505Z_31h2rpXp92we4vWe.json.gz.75f1176f'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail/us-east-1/2021/12/30/859569241075_CloudTrail_us-east-1_20211230T1510Z_1Ks3EmmUEJdaiP6Z.json.gz.3f94dCd9'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail/us-east-1/2021/12/30/859569241075_CloudTrail_us-east-1_20211230T1515Z_KfFTNsPgq1nitBWb.json.gz.f86Fea8b'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail/us-east-1/2021/12/30/859569241075_CloudTrail_us-east-1_20211230T1520Z_JN0d9psAcuvwcxHW.json.gz.6fEDEf8F'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail/us-east-1/2021/12/30/859569241075_CloudTrail_us-east-1_20211230T1520Z_Tu4DmqFiULWOZAOZ.json.gz.5FDb6B94'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail/us-east-1/2021/12/30/859569241075_CloudTrail_us-east-1_20211230T1535Z_XBHLUNwIKYSd7qst.json.gz.E4bDeC63'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail/us-east-1/2021/12/30/859569241075_CloudTrail_us-east-1_20211230T1535Z_d20ltdcww4QL5hTK.json.gz.924DebdA'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail/us-east-1/2021/12/30/859569241075_CloudTrail_us-east-1_20211230T1535Z_imZIy4LHyNUbWAG4.json.gz.4b1E2e30'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail/us-east-1/2021/12/30/859569241075_CloudTrail_us-east-1_20211230T1540Z_vCeAAiXqn2O5kLN3.json.gz.6f9a7b2d'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail/us-east-1/2021/12/30/859569241075_CloudTrail_us-east-1_20211230T1545Z_4fdIsdHH2n35FCvp.json.gz.fADdd6C1'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail/us-east-2/2021/12/30/859569241075_CloudTrail_us-east-2_20211230T1510Z_7vbA6rXdtIWerliC.json.gz.ED14CB76'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail/us-east-2/2021/12/30/859569241075_CloudTrail_us-east-2_20211230T1510Z_SdnDAwkko6smSvX3.json.gz.9F6d21A1'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail/us-east-2/2021/12/30/859569241075_CloudTrail_us-east-2_20211230T1515Z_8BZzwTngY9cbEuJ1.json.gz.4C6BfAB4'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail/us-east-2/2021/12/30/859569241075_CloudTrail_us-east-2_20211230T1515Z_GV4FhfAxhBvs25ZJ.json.gz.3c514ceA'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail/us-east-2/2021/12/30/859569241075_CloudTrail_us-east-2_20211230T1515Z_nvSkRoo05ZEkGgYf.json.gz.a1AeaC3D'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail/us-east-2/2021/12/30/859569241075_CloudTrail_us-east-2_20211230T1520Z_6skH1TmKSpi4d9Sd.json.gz.a7a7551F'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail/us-east-2/2021/12/30/859569241075_CloudTrail_us-east-2_20211230T1525Z_SXV28V4BHlPy9ipq.json.gz.b3e6e1aB'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail/us-east-2/2021/12/30/859569241075_CloudTrail_us-east-2_20211230T1525Z_ixEXFEo4KQtFgvxO.json.gz.589Ca23a'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail/us-east-2/2021/12/30/859569241075_CloudTrail_us-east-2_20211230T1535Z_UGcFQoiGw2830zzO.json.gz.780fcFd0'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail/us-east-2/2021/12/30/859569241075_CloudTrail_us-east-2_20211230T1535Z_ZpaePTzntpVFyF3j.json.gz.A739DADC'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail/us-east-2/2021/12/30/859569241075_CloudTrail_us-east-2_20211230T1540Z_fll5CGgtPHwt1JQn.json.gz.18Ce366B'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail/us-east-2/2021/12/30/859569241075_CloudTrail_us-east-2_20211230T1545Z_H4SI4H0KHycAKigz.json.gz.FaB5CF99'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail/us-east-2/2021/12/30/859569241075_CloudTrail_us-east-2_20211230T1545Z_dKyqNtpZU6UTNjU5.json.gz.Dede14bD'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail/us-west-2/2021/12/30/859569241075_CloudTrail_us-west-2_20211230T1510Z_60dX2T6WKUVd4992.json.gz.9DD82ed9'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail/us-west-2/2021/12/30/859569241075_CloudTrail_us-west-2_20211230T1535Z_zFteSj566r0cv4l0.json.gz.dc7cb941'\n",
      "[Errno 2] No such file or directory: './anoc001-test-mauriciobarojas/AWSLogs/859569241075/CloudTrail/us-west-2/2021/12/30/859569241075_CloudTrail_us-west-2_20211230T1540Z_8WS6MPDfrvkBAcne.json.gz.d46dc0FE'\n",
      "Finalizo la descarga del Bucket : anoc001-test-mauriciobarojas\n",
      "Se creo zip del Bucket : anoc001-test-mauriciobarojas\n",
      "Se limpio la carpeta : anoc001-test-mauriciobarojas\n"
     ]
    }
   ],
   "source": [
    "\"\"\" PASO VII,VIII,IX\n",
    "    Descargar arhivos de los Bucket\n",
    "    Referencia:\n",
    "    https://stackoverflow.com/questions/31918960/boto3-to-download-all-files-from-a-s3-bucket\n",
    "    https://stackoverflow.com/questions/68371889/how-to-fix-key-error-in-listing-content-from-a-s3-bucket-in-aws       \n",
    "\"\"\"\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "bucket1= 'anoc001-test-data-mauriciobarojas'\n",
    "bucket2= 'anoc001-test-mauriciobarojas'\n",
    "\n",
    "\"\"\"PASO VII.1  bucket anoc001-test-data-mauriciobarojas\"\"\"\n",
    "dir = os.path.join('./',str(bucket1))\n",
    "if os.path.exists(dir):\n",
    "    shutil.rmtree(dir)    \n",
    "    os.makedirs(dir)\n",
    "else: os.makedirs(dir)   \n",
    "\n",
    "\"\"\"PASO VIII\"\"\"\n",
    "\n",
    "s3=boto3.client('s3')\n",
    "#s3=boto3.client('s3')\n",
    "#list=s3.list_objects(Bucket='anoc001-test-211228')['Contents']\n",
    "listObject=s3.list_objects(Bucket=str(bucket1))\n",
    "if 'Contents' in listObject:\n",
    "    list=s3.list_objects(Bucket=str(bucket1))['Contents']\n",
    "    for s3_key in list:\n",
    "        s3_object = s3_key['Key']\n",
    "        if not s3_object.endswith(\"/\"):\n",
    "            s3.download_file(str(bucket1), s3_object,os.path.join(dir,s3_object))\n",
    "        else:\n",
    "            if not os.path.exists(s3_object):\n",
    "                os.makedirs(os.path.join(dir,s3_object))\n",
    "\n",
    "    #shutil.make_archive(dir,'zip',dir)\n",
    "    print(\"Finalizo la descarga del Bucket : \" +str(bucket1))\n",
    "else:\n",
    "    print(\"Sin Archivos en el Bucket : \" +str(bucket1))\n",
    "    \n",
    "shutil.make_archive(dir,'zip',dir)\n",
    "print(\"Se creo zip del Bucket : \" +str(bucket1))\n",
    "\n",
    "shutil.rmtree(dir)\n",
    "print(\"Se limpio la carpeta : \" +str(bucket1))\n",
    "    \n",
    "\n",
    "\"\"\"PASO VII.2  bucket anoc001-test-mauriciobarojas\"\"\"\n",
    "\n",
    "dir = os.path.join('./',str(bucket2))\n",
    "if os.path.exists(dir):\n",
    "    shutil.rmtree(dir)    \n",
    "    os.makedirs(dir)\n",
    "else: os.makedirs(dir)\n",
    "\n",
    "\"\"\"PASO IX\"\"\"\n",
    "#s3=boto3.client('s3')\n",
    "#list=s3.list_objects(Bucket='anoc001-test-211228')['Contents']\n",
    "listObject=s3.list_objects(Bucket=str(bucket2))\n",
    "if 'Contents' in listObject:\n",
    "    list=s3.list_objects(Bucket=str(bucket2))['Contents']\n",
    "    for s3_key in list:\n",
    "        s3_object = s3_key['Key']\n",
    "        if not s3_object.endswith(\"/\"):\n",
    "            try:\n",
    "                s3.download_file(str(bucket2), s3_object,os.path.join(dir,s3_object))\n",
    "            except Exception as inst:\n",
    "                print(inst)    \n",
    "        else:\n",
    "            if not os.path.exists(s3_object):\n",
    "                os.makedirs(os.path.join(dir,s3_object))\n",
    "\n",
    "    #shutil.make_archive(dir,'zip',dir)\n",
    "    print(\"Finalizo la descarga del Bucket : \" +str(bucket2))\n",
    "else:\n",
    "    print(\"Sin Archivos en el Bucket : \" +str(bucket2))\n",
    "    \n",
    "shutil.make_archive(dir,'zip',dir)\n",
    "print(\"Se creo zip del Bucket : \" +str(bucket2))\n",
    "\n",
    "shutil.rmtree(dir)\n",
    "print(\"Se limpio la carpeta : \" +str(bucket2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bd73cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Mediante este codigo no se pudo descargar el contenido de la carpeta se tuvo que cambiar la por la opcion de linea de comando de awd cli\"\"\"\n",
    "\n",
    "aws s3 cp s3://anoc001-test-mauriciobarojas/AWSLogs D:\\USER1\\ResultadoEjercicio1\\anoc001-test-mauriciobarojas --recursive"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
